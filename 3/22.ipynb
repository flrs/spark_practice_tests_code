{"cells": [{"cell_type": "markdown", "source": ["# Test 3, Question 22\n> **Hint:** In Databricks, import code for all questions via this URL:\n> \n> https://github.com/flrs/spark_practice_tests_code/raw/main/spark_practice_tests_code.dbc"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "87a4339f-cc26-4446-83b0-5fd419c20c70"}}}, {"cell_type": "code", "source": ["%run ./create_itemsDf"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "ce0bf962-8af0-4c63-8bed-7acbfb428c56"}}, "outputs": [{"output_type": "display_data", "metadata": {"application/vnd.databricks.v1+output": {"datasetInfos": [], "data": "<div class=\"ansiout\"></div>", "removedWidgets": [], "addedWidgets": {}, "metadata": {}, "type": "html", "arguments": {}}}, "data": {"text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}], "execution_count": 0}, {"cell_type": "code", "source": ["itemsDf.show()"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "2cf0023c-9979-47cc-9479-179fbb5dea21"}}, "outputs": [{"output_type": "display_data", "metadata": {"application/vnd.databricks.v1+output": {"datasetInfos": [], "data": "<div class=\"ansiout\">+------+--------------------+-------------------+\n|itemId|          attributes|           supplier|\n+------+--------------------+-------------------+\n|     1|[blue, winter, cozy]|Sports Company Inc.|\n|     2|[red, summer, fre...|              YetiX|\n|     3|[green, summer, t...|Sports Company Inc.|\n+------+--------------------+-------------------+\n\n</div>", "removedWidgets": [], "addedWidgets": {}, "metadata": {}, "type": "html", "arguments": {}}}, "data": {"text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+--------------------+-------------------+\nitemId|          attributes|           supplier|\n+------+--------------------+-------------------+\n     1|[blue, winter, cozy]|Sports Company Inc.|\n     2|[red, summer, fre...|              YetiX|\n     3|[green, summer, t...|Sports Company Inc.|\n+------+--------------------+-------------------+\n\n</div>"]}}], "execution_count": 0}, {"cell_type": "markdown", "source": ["## Answer 1 (incorrect)"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "8fef6132-53bf-4c40-b876-e17972c28fe8"}}}, {"cell_type": "code", "source": ["counter = 0\n\nfor index, row in itemsDf.iterrows():\n  if 'Inc.' in row['supplier']:\n    counter = counter + 1\n    \nprint(counter)"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "4a1dfb7c-aaf8-4c46-89c0-7367cb77a413"}}, "outputs": [{"output_type": "display_data", "metadata": {"application/vnd.databricks.v1+output": {"datasetInfos": [], "data": "<div class=\"ansiout\"></div>", "removedWidgets": [], "addedWidgets": {}, "metadata": {}, "type": "html", "arguments": {}}}, "data": {"text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}, {"output_type": "display_data", "metadata": {"application/vnd.databricks.v1+output": {"data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-85109432458286&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> counter <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-cyan-fg\">0</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">for</span> index<span class=\"ansi-blue-fg\">,</span> row <span class=\"ansi-green-fg\">in</span> itemsDf<span class=\"ansi-blue-fg\">.</span>iterrows<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span>   <span class=\"ansi-green-fg\">if</span> <span class=\"ansi-blue-fg\">&#39;Inc.&#39;</span> <span class=\"ansi-green-fg\">in</span> row<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#39;supplier&#39;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span>     counter <span class=\"ansi-blue-fg\">=</span> counter <span class=\"ansi-blue-fg\">+</span> <span class=\"ansi-cyan-fg\">1</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">__getattr__</span><span class=\"ansi-blue-fg\">(self, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1664</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">   1665</span>         <span class=\"ansi-green-fg\">if</span> name <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">in</span> self<span class=\"ansi-blue-fg\">.</span>columns<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1666</span><span class=\"ansi-red-fg\">             raise AttributeError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1667</span>                 &#34;&#39;%s&#39; object has no attribute &#39;%s&#39;&#34; % (self.__class__.__name__, name))\n<span class=\"ansi-green-intense-fg ansi-bold\">   1668</span>         jc <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>apply<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AttributeError</span>: &#39;DataFrame&#39; object has no attribute &#39;iterrows&#39;</div>", "errorSummary": "<span class=\"ansi-red-fg\">AttributeError</span>: &#39;DataFrame&#39; object has no attribute &#39;iterrows&#39;", "metadata": {}, "errorTraceType": "html", "type": "ipynbError", "arguments": {}}}, "data": {"text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-85109432458286&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> counter <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-cyan-fg\">0</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">for</span> index<span class=\"ansi-blue-fg\">,</span> row <span class=\"ansi-green-fg\">in</span> itemsDf<span class=\"ansi-blue-fg\">.</span>iterrows<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span>   <span class=\"ansi-green-fg\">if</span> <span class=\"ansi-blue-fg\">&#39;Inc.&#39;</span> <span class=\"ansi-green-fg\">in</span> row<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#39;supplier&#39;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span>     counter <span class=\"ansi-blue-fg\">=</span> counter <span class=\"ansi-blue-fg\">+</span> <span class=\"ansi-cyan-fg\">1</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">__getattr__</span><span class=\"ansi-blue-fg\">(self, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1664</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">   1665</span>         <span class=\"ansi-green-fg\">if</span> name <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">in</span> self<span class=\"ansi-blue-fg\">.</span>columns<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1666</span><span class=\"ansi-red-fg\">             raise AttributeError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1667</span>                 &#34;&#39;%s&#39; object has no attribute &#39;%s&#39;&#34; % (self.__class__.__name__, name))\n<span class=\"ansi-green-intense-fg ansi-bold\">   1668</span>         jc <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>apply<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AttributeError</span>: &#39;DataFrame&#39; object has no attribute &#39;iterrows&#39;</div>"]}}], "execution_count": 0}, {"cell_type": "markdown", "source": ["## Answer 2 (incorrect)"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "9edca3fe-7135-4cbe-a61a-a5dbd2d4c733"}}}, {"cell_type": "code", "source": ["counter = 0\n\ndef count(x):\n  if 'Inc.' in x['supplier']:\n    counter = counter + 1\n\nitemsDf.foreach(count)\nprint(counter)"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "6c3f712f-0536-4ce8-ad81-b36c5ec71945"}}, "outputs": [{"output_type": "display_data", "metadata": {"application/vnd.databricks.v1+output": {"datasetInfos": [], "data": "<div class=\"ansiout\"></div>", "removedWidgets": [], "addedWidgets": {}, "metadata": {}, "type": "html", "arguments": {}}}, "data": {"text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}, {"output_type": "display_data", "metadata": {"application/vnd.databricks.v1+output": {"data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-85109432458288&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span>     counter <span class=\"ansi-blue-fg\">=</span> counter <span class=\"ansi-blue-fg\">+</span> <span class=\"ansi-cyan-fg\">1</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> \n<span class=\"ansi-green-fg\">----&gt; 7</span><span class=\"ansi-red-fg\"> </span>itemsDf<span class=\"ansi-blue-fg\">.</span>foreach<span class=\"ansi-blue-fg\">(</span>count<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      8</span> print<span class=\"ansi-blue-fg\">(</span>counter<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">foreach</span><span class=\"ansi-blue-fg\">(self, f)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    782</span>         <span class=\"ansi-blue-fg\">&gt;&gt;</span><span class=\"ansi-blue-fg\">&gt;</span> df<span class=\"ansi-blue-fg\">.</span>foreach<span class=\"ansi-blue-fg\">(</span>f<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    783</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 784</span><span class=\"ansi-red-fg\">         </span>self<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">.</span>foreach<span class=\"ansi-blue-fg\">(</span>f<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    785</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    786</span>     <span class=\"ansi-green-fg\">def</span> foreachPartition<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> f<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">foreach</span><span class=\"ansi-blue-fg\">(self, f)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    920</span>                 f<span class=\"ansi-blue-fg\">(</span>x<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    921</span>             <span class=\"ansi-green-fg\">return</span> iter<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">--&gt; 922</span><span class=\"ansi-red-fg\">         </span>self<span class=\"ansi-blue-fg\">.</span>mapPartitions<span class=\"ansi-blue-fg\">(</span>processPartition<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>count<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>  <span class=\"ansi-red-fg\"># Force evaluation</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    923</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    924</span>     <span class=\"ansi-green-fg\">def</span> foreachPartition<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> f<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">count</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1268</span>         <span class=\"ansi-cyan-fg\">3</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1269</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">-&gt; 1270</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>mapPartitions<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> i<span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">[</span>sum<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1</span> <span class=\"ansi-green-fg\">for</span> _ <span class=\"ansi-green-fg\">in</span> i<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>sum<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1271</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1272</span>     <span class=\"ansi-green-fg\">def</span> stats<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">sum</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1257</span>         <span class=\"ansi-cyan-fg\">6.0</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1258</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">-&gt; 1259</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>mapPartitions<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> x<span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">[</span>sum<span class=\"ansi-blue-fg\">(</span>x<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>fold<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">,</span> operator<span class=\"ansi-blue-fg\">.</span>add<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1260</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1261</span>     <span class=\"ansi-green-fg\">def</span> count<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">fold</span><span class=\"ansi-blue-fg\">(self, zeroValue, op)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1111</span>         <span class=\"ansi-red-fg\"># zeroValue provided to each partition is unique from the one provided</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1112</span>         <span class=\"ansi-red-fg\"># to the final reduce call</span>\n<span class=\"ansi-green-fg\">-&gt; 1113</span><span class=\"ansi-red-fg\">         </span>vals <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>mapPartitions<span class=\"ansi-blue-fg\">(</span>func<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>collect<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1114</span>         <span class=\"ansi-green-fg\">return</span> reduce<span class=\"ansi-blue-fg\">(</span>op<span class=\"ansi-blue-fg\">,</span> vals<span class=\"ansi-blue-fg\">,</span> zeroValue<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1115</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">collect</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    965</span>         <span class=\"ansi-red-fg\"># Default path used in OSS Spark / for non-credential passthrough clusters:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    966</span>         <span class=\"ansi-green-fg\">with</span> SCCallSiteSync<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>context<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> css<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 967</span><span class=\"ansi-red-fg\">             </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>ctx<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonRDD<span class=\"ansi-blue-fg\">.</span>collectAndServe<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    968</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_jrdd_deserializer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    969</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 117</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 14.0 failed 1 times, most recent failure: Lost task 2.0 in stage 14.0 (TID 98) (ip-10-172-178-96.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: &#39;UnboundLocalError: local variable &#39;counter&#39; referenced before assignment&#39;, from &lt;command-85109432458288&gt;, line 5. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 713, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 703, in process\n    out_iter = func(split_index, iterator)\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 2953, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 2953, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 2953, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 419, in func\n    return f(iterator)\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 920, in processPartition\n    f(x)\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 72, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-85109432458288&gt;&#34;, line 5, in count\nUnboundLocalError: local variable &#39;counter&#39; referenced before assignment\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:661)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:813)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:795)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:614)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1038)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:788)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1643)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:791)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:647)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2765)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2712)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2706)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2706)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1255)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1255)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1255)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2914)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2902)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1028)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2446)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1036)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1034)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:260)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: &#39;UnboundLocalError: local variable &#39;counter&#39; referenced before assignment&#39;, from &lt;command-85109432458288&gt;, line 5. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 713, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 703, in process\n    out_iter = func(split_index, iterator)\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 2953, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 2953, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 2953, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 419, in func\n    return f(iterator)\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 920, in processPartition\n    f(x)\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 72, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-85109432458288&gt;&#34;, line 5, in count\nUnboundLocalError: local variable &#39;counter&#39; referenced before assignment\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:661)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:813)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:795)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:614)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1038)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:788)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1643)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:791)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:647)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>", "errorSummary": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 14.0 failed 1 times, most recent failure: Lost task 2.0 in stage 14.0 (TID 98) (ip-10-172-178-96.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: &#39;UnboundLocalError: local variable &#39;counter&#39; referenced before assignment&#39;, from &lt;command-85109432458288&gt;, line 5. Full traceback below:", "metadata": {}, "errorTraceType": "html", "type": "ipynbError", "arguments": {}}}, "data": {"text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-85109432458288&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span>     counter <span class=\"ansi-blue-fg\">=</span> counter <span class=\"ansi-blue-fg\">+</span> <span class=\"ansi-cyan-fg\">1</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> \n<span class=\"ansi-green-fg\">----&gt; 7</span><span class=\"ansi-red-fg\"> </span>itemsDf<span class=\"ansi-blue-fg\">.</span>foreach<span class=\"ansi-blue-fg\">(</span>count<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      8</span> print<span class=\"ansi-blue-fg\">(</span>counter<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">foreach</span><span class=\"ansi-blue-fg\">(self, f)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    782</span>         <span class=\"ansi-blue-fg\">&gt;&gt;</span><span class=\"ansi-blue-fg\">&gt;</span> df<span class=\"ansi-blue-fg\">.</span>foreach<span class=\"ansi-blue-fg\">(</span>f<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    783</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 784</span><span class=\"ansi-red-fg\">         </span>self<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">.</span>foreach<span class=\"ansi-blue-fg\">(</span>f<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    785</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    786</span>     <span class=\"ansi-green-fg\">def</span> foreachPartition<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> f<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">foreach</span><span class=\"ansi-blue-fg\">(self, f)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    920</span>                 f<span class=\"ansi-blue-fg\">(</span>x<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    921</span>             <span class=\"ansi-green-fg\">return</span> iter<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">--&gt; 922</span><span class=\"ansi-red-fg\">         </span>self<span class=\"ansi-blue-fg\">.</span>mapPartitions<span class=\"ansi-blue-fg\">(</span>processPartition<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>count<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>  <span class=\"ansi-red-fg\"># Force evaluation</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    923</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    924</span>     <span class=\"ansi-green-fg\">def</span> foreachPartition<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> f<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">count</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1268</span>         <span class=\"ansi-cyan-fg\">3</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1269</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">-&gt; 1270</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>mapPartitions<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> i<span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">[</span>sum<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1</span> <span class=\"ansi-green-fg\">for</span> _ <span class=\"ansi-green-fg\">in</span> i<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>sum<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1271</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1272</span>     <span class=\"ansi-green-fg\">def</span> stats<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">sum</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1257</span>         <span class=\"ansi-cyan-fg\">6.0</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1258</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">-&gt; 1259</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>mapPartitions<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> x<span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">[</span>sum<span class=\"ansi-blue-fg\">(</span>x<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>fold<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">,</span> operator<span class=\"ansi-blue-fg\">.</span>add<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1260</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1261</span>     <span class=\"ansi-green-fg\">def</span> count<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">fold</span><span class=\"ansi-blue-fg\">(self, zeroValue, op)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1111</span>         <span class=\"ansi-red-fg\"># zeroValue provided to each partition is unique from the one provided</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1112</span>         <span class=\"ansi-red-fg\"># to the final reduce call</span>\n<span class=\"ansi-green-fg\">-&gt; 1113</span><span class=\"ansi-red-fg\">         </span>vals <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>mapPartitions<span class=\"ansi-blue-fg\">(</span>func<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>collect<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1114</span>         <span class=\"ansi-green-fg\">return</span> reduce<span class=\"ansi-blue-fg\">(</span>op<span class=\"ansi-blue-fg\">,</span> vals<span class=\"ansi-blue-fg\">,</span> zeroValue<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1115</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">collect</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    965</span>         <span class=\"ansi-red-fg\"># Default path used in OSS Spark / for non-credential passthrough clusters:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    966</span>         <span class=\"ansi-green-fg\">with</span> SCCallSiteSync<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>context<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> css<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 967</span><span class=\"ansi-red-fg\">             </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>ctx<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonRDD<span class=\"ansi-blue-fg\">.</span>collectAndServe<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    968</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_jrdd_deserializer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    969</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 117</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 14.0 failed 1 times, most recent failure: Lost task 2.0 in stage 14.0 (TID 98) (ip-10-172-178-96.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: &#39;UnboundLocalError: local variable &#39;counter&#39; referenced before assignment&#39;, from &lt;command-85109432458288&gt;, line 5. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 713, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 703, in process\n    out_iter = func(split_index, iterator)\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 2953, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 2953, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 2953, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 419, in func\n    return f(iterator)\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 920, in processPartition\n    f(x)\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 72, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-85109432458288&gt;&#34;, line 5, in count\nUnboundLocalError: local variable &#39;counter&#39; referenced before assignment\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:661)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:813)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:795)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:614)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1038)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:788)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1643)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:791)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:647)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2765)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2712)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2706)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2706)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1255)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1255)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1255)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2914)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2902)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1028)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2446)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1036)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1034)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:260)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: &#39;UnboundLocalError: local variable &#39;counter&#39; referenced before assignment&#39;, from &lt;command-85109432458288&gt;, line 5. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 713, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 703, in process\n    out_iter = func(split_index, iterator)\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 2953, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 2953, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 2953, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 419, in func\n    return f(iterator)\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 920, in processPartition\n    f(x)\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 72, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-85109432458288&gt;&#34;, line 5, in count\nUnboundLocalError: local variable &#39;counter&#39; referenced before assignment\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:661)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:813)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:795)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:614)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1038)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:788)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1643)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:791)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:647)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>"]}}], "execution_count": 0}, {"cell_type": "markdown", "source": ["## Answer 3 (incorrect)"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "9e7b8a51-bd8b-45ce-aed6-488cc351332a"}}}, {"cell_type": "code", "source": ["print(itemsDf.foreach(lambda x: 'Inc.' in x))"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "14f9adb3-dc1d-4e18-8cfa-4fddecdfdfcd"}}, "outputs": [{"output_type": "display_data", "metadata": {"application/vnd.databricks.v1+output": {"datasetInfos": [], "data": "<div class=\"ansiout\">None\n</div>", "removedWidgets": [], "addedWidgets": {}, "metadata": {}, "type": "html", "arguments": {}}}, "data": {"text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">None\n</div>"]}}], "execution_count": 0}, {"cell_type": "markdown", "source": ["## Answer 4 (incorrect)"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "3da08153-8385-45d8-85f4-9d1e5156ec37"}}}, {"cell_type": "code", "source": ["print(itemsDf.foreach(lambda x: 'Inc.' in x).sum())"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "741846ba-74a7-4878-9715-49e613b7b7e2"}}, "outputs": [{"output_type": "display_data", "metadata": {"application/vnd.databricks.v1+output": {"datasetInfos": [], "data": "<div class=\"ansiout\"></div>", "removedWidgets": [], "addedWidgets": {}, "metadata": {}, "type": "html", "arguments": {}}}, "data": {"text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}, {"output_type": "display_data", "metadata": {"application/vnd.databricks.v1+output": {"data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-85109432458290&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>print<span class=\"ansi-blue-fg\">(</span>itemsDf<span class=\"ansi-blue-fg\">.</span>foreach<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> x<span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">&#39;Inc.&#39;</span> <span class=\"ansi-green-fg\">in</span> x<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>sum<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AttributeError</span>: &#39;NoneType&#39; object has no attribute &#39;sum&#39;</div>", "errorSummary": "<span class=\"ansi-red-fg\">AttributeError</span>: &#39;NoneType&#39; object has no attribute &#39;sum&#39;", "metadata": {}, "errorTraceType": "html", "type": "ipynbError", "arguments": {}}}, "data": {"text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-85109432458290&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>print<span class=\"ansi-blue-fg\">(</span>itemsDf<span class=\"ansi-blue-fg\">.</span>foreach<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> x<span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">&#39;Inc.&#39;</span> <span class=\"ansi-green-fg\">in</span> x<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>sum<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AttributeError</span>: &#39;NoneType&#39; object has no attribute &#39;sum&#39;</div>"]}}], "execution_count": 0}, {"cell_type": "markdown", "source": ["## Answer 5 (correct)"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "2f909ada-99cb-406f-8df0-0f31d52f554c"}}}, {"cell_type": "code", "source": ["counter=sc.accumulator(0)\n\ndef check_if_inc_in_supplier(row):\n  if 'Inc.' in row['supplier']:\n    counter.add(1)\n\nitemsDf.foreach(check_if_inc_in_supplier)\nprint(counter.value)"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "23ad35b9-706e-47f7-8c55-ea52ba709d55"}}, "outputs": [{"output_type": "display_data", "metadata": {"application/vnd.databricks.v1+output": {"datasetInfos": [], "data": "<div class=\"ansiout\">2\n</div>", "removedWidgets": [], "addedWidgets": {}, "metadata": {}, "type": "html", "arguments": {}}}, "data": {"text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">2\n</div>"]}}], "execution_count": 0}], "metadata": {"application/vnd.databricks.v1+notebook": {"notebookName": "142", "dashboards": [], "notebookMetadata": {"pythonIndentUnit": 2}, "language": "python", "widgets": {}, "notebookOrigID": 85109432458278}}, "nbformat": 4, "nbformat_minor": 0}