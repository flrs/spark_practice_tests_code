{"cells": [{"cell_type": "markdown", "source": ["# Test 3, Question 19\n> **Hint:** In Databricks, import code for all questions via this URL:\n> \n> https://github.com/flrs/spark_practice_tests_code/raw/main/spark_practice_tests_code.dbc"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "87a4339f-cc26-4446-83b0-5fd419c20c70"}}}, {"cell_type": "code", "source": ["%run ./create_itemsDf"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "ce0bf962-8af0-4c63-8bed-7acbfb428c56"}}, "outputs": [], "execution_count": 0}, {"cell_type": "code", "source": ["itemsDf.show(truncate=False)\nitemsDf.printSchema()"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "ae50de9e-30ce-4c67-a0ae-bf60cf25999c"}}, "outputs": [{"output_type": "display_data", "metadata": {"application/vnd.databricks.v1+output": {"datasetInfos": [], "data": "<div class=\"ansiout\">+------+-----------------------------+-------------------+\n|itemId|attributes                   |supplier           |\n+------+-----------------------------+-------------------+\n|1     |[blue, winter, cozy]         |Sports Company Inc.|\n|2     |[red, summer, fresh, cooling]|YetiX              |\n|3     |[green, summer, travel]      |Sports Company Inc.|\n+------+-----------------------------+-------------------+\n\nroot\n |-- itemId: integer (nullable = true)\n |-- attributes: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- supplier: string (nullable = true)\n\n</div>", "removedWidgets": [], "addedWidgets": {}, "metadata": {}, "type": "html", "arguments": {}}}, "data": {"text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+-----------------------------+-------------------+\nitemId|attributes                   |supplier           |\n+------+-----------------------------+-------------------+\n1     |[blue, winter, cozy]         |Sports Company Inc.|\n2     |[red, summer, fresh, cooling]|YetiX              |\n3     |[green, summer, travel]      |Sports Company Inc.|\n+------+-----------------------------+-------------------+\n\nroot\n-- itemId: integer (nullable = true)\n-- attributes: array (nullable = true)\n    |-- element: string (containsNull = true)\n-- supplier: string (nullable = true)\n\n</div>"]}}, {"output_type": "display_data", "metadata": {"application/vnd.databricks.v1+output": {"datasetInfos": [], "data": "<div class=\"ansiout\"></div>", "removedWidgets": [], "addedWidgets": {}, "metadata": {}, "type": "html", "arguments": {}}}, "data": {"text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}], "execution_count": 0}, {"cell_type": "code", "source": ["filePath = '/FileStore/itemsDf.parquet'\n\nitemsDf.write.format(\"parquet\").mode(\"overwrite\").save(filePath)"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "2907363c-bb01-4e0d-8c5f-87449955ced3"}}, "outputs": [{"output_type": "display_data", "metadata": {"application/vnd.databricks.v1+output": {"datasetInfos": [], "data": "<div class=\"ansiout\"></div>", "removedWidgets": [], "addedWidgets": {}, "metadata": {}, "type": "html", "arguments": {}}}, "data": {"text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}], "execution_count": 0}, {"cell_type": "markdown", "source": ["## Answer 1 (incorrect)"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "9e7b8a51-bd8b-45ce-aed6-488cc351332a"}}}, {"cell_type": "code", "source": ["from pyspark.sql.types import StructType, StructField, IntegerType, ArrayType, StringType\n\nitemsDfSchema = StructType([\n  StructField(\"itemId\", IntegerType()),\n  StructField(\"attributes\", StringType()),\n  StructField(\"supplier\", StringType())])\n\nitemsDf = spark.read.schema(itemsDfSchema).parquet(filePath)\n\nitemsDf.show(truncate=False)\nitemsDf.printSchema()"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "14f9adb3-dc1d-4e18-8cfa-4fddecdfdfcd"}}, "outputs": [{"output_type": "display_data", "metadata": {"application/vnd.databricks.v1+output": {"datasetInfos": [], "data": "<div class=\"ansiout\"></div>", "removedWidgets": [], "addedWidgets": {}, "metadata": {}, "type": "html", "arguments": {}}}, "data": {"text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}, {"output_type": "display_data", "metadata": {"application/vnd.databricks.v1+output": {"data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4039916869340883&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      8</span> itemsDf <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>schema<span class=\"ansi-blue-fg\">(</span>itemsDfSchema<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>parquet<span class=\"ansi-blue-fg\">(</span>filePath<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      9</span> \n<span class=\"ansi-green-fg\">---&gt; 10</span><span class=\"ansi-red-fg\"> </span>itemsDf<span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">False</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     11</span> itemsDf<span class=\"ansi-blue-fg\">.</span>printSchema<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">show</span><span class=\"ansi-blue-fg\">(self, n, truncate, vertical)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    490</span>             print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">20</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    491</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 492</span><span class=\"ansi-red-fg\">             </span>print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> int<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    493</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    494</span>     <span class=\"ansi-green-fg\">def</span> __repr__<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 117</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o723.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 20.0 failed 1 times, most recent failure: Lost task 0.0 in stage 20.0 (TID 57) (ip-10-172-179-133.us-west-2.compute.internal executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/itemsDf.parquet/part-00007-tid-2263344001468129208-3575ab9f-2487-4d5e-bd57-2b32b9f64dfc-52-1-c000.snappy.parquet. Parquet column cannot be converted. Column: [attributes, list, element], Expected: ByteType, Found: BINARY\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:389)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:359)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$prepareNextFile$1(FileScanRDD.scala:559)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:68)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:54)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:101)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\n\tat com.databricks.sql.io.parquet.VectorizedColumnReader.constructConvertNotSupportedException(VectorizedColumnReader.java:448)\n\tat com.databricks.sql.io.parquet.VectorizedColumnReader.readBinaryBatch(VectorizedColumnReader.java:827)\n\tat com.databricks.sql.io.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:406)\n\tat com.databricks.sql.io.parquet.VectorizedArrayReader.readBatch(VectorizedArrayReader.java:119)\n\tat com.databricks.sql.io.parquet.DatabricksVectorizedParquetRecordReader.nextBatch(DatabricksVectorizedParquetRecordReader.java:480)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:190)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:333)\n\t... 18 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2765)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2712)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2706)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2706)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1255)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1255)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1255)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2914)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2902)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1028)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2446)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:289)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:299)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:88)\n\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:75)\n\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:62)\n\tat org.apache.spark.sql.execution.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:512)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:511)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:399)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollectResult(limit.scala:59)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3018)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3810)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2742)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3802)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:126)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:267)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:852)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:217)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3800)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2742)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2949)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:306)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:343)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/itemsDf.parquet/part-00007-tid-2263344001468129208-3575ab9f-2487-4d5e-bd57-2b32b9f64dfc-52-1-c000.snappy.parquet. Parquet column cannot be converted. Column: [attributes, list, element], Expected: ByteType, Found: BINARY\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:389)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:359)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$prepareNextFile$1(FileScanRDD.scala:559)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:68)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:54)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:101)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\n\tat com.databricks.sql.io.parquet.VectorizedColumnReader.constructConvertNotSupportedException(VectorizedColumnReader.java:448)\n\tat com.databricks.sql.io.parquet.VectorizedColumnReader.readBinaryBatch(VectorizedColumnReader.java:827)\n\tat com.databricks.sql.io.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:406)\n\tat com.databricks.sql.io.parquet.VectorizedArrayReader.readBatch(VectorizedArrayReader.java:119)\n\tat com.databricks.sql.io.parquet.DatabricksVectorizedParquetRecordReader.nextBatch(DatabricksVectorizedParquetRecordReader.java:480)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:190)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:333)\n\t... 18 more\n</div>", "errorSummary": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 20.0 failed 1 times, most recent failure: Lost task 0.0 in stage 20.0 (TID 57) (ip-10-172-179-133.us-west-2.compute.internal executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/itemsDf.parquet/part-00007-tid-2263344001468129208-3575ab9f-2487-4d5e-bd57-2b32b9f64dfc-52-1-c000.snappy.parquet. Parquet column cannot be converted. Column: [attributes, list, element], Expected: ByteType, Found: BINARY", "metadata": {}, "errorTraceType": "html", "type": "ipynbError", "arguments": {}}}, "data": {"text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4039916869340883&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      8</span> itemsDf <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>schema<span class=\"ansi-blue-fg\">(</span>itemsDfSchema<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>parquet<span class=\"ansi-blue-fg\">(</span>filePath<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      9</span> \n<span class=\"ansi-green-fg\">---&gt; 10</span><span class=\"ansi-red-fg\"> </span>itemsDf<span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">False</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     11</span> itemsDf<span class=\"ansi-blue-fg\">.</span>printSchema<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">show</span><span class=\"ansi-blue-fg\">(self, n, truncate, vertical)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    490</span>             print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">20</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    491</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 492</span><span class=\"ansi-red-fg\">             </span>print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> int<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    493</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    494</span>     <span class=\"ansi-green-fg\">def</span> __repr__<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 117</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o723.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 20.0 failed 1 times, most recent failure: Lost task 0.0 in stage 20.0 (TID 57) (ip-10-172-179-133.us-west-2.compute.internal executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/itemsDf.parquet/part-00007-tid-2263344001468129208-3575ab9f-2487-4d5e-bd57-2b32b9f64dfc-52-1-c000.snappy.parquet. Parquet column cannot be converted. Column: [attributes, list, element], Expected: ByteType, Found: BINARY\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:389)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:359)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$prepareNextFile$1(FileScanRDD.scala:559)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:68)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:54)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:101)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\n\tat com.databricks.sql.io.parquet.VectorizedColumnReader.constructConvertNotSupportedException(VectorizedColumnReader.java:448)\n\tat com.databricks.sql.io.parquet.VectorizedColumnReader.readBinaryBatch(VectorizedColumnReader.java:827)\n\tat com.databricks.sql.io.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:406)\n\tat com.databricks.sql.io.parquet.VectorizedArrayReader.readBatch(VectorizedArrayReader.java:119)\n\tat com.databricks.sql.io.parquet.DatabricksVectorizedParquetRecordReader.nextBatch(DatabricksVectorizedParquetRecordReader.java:480)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:190)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:333)\n\t... 18 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2765)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2712)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2706)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2706)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1255)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1255)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1255)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2914)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2902)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1028)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2446)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:289)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:299)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:88)\n\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:75)\n\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:62)\n\tat org.apache.spark.sql.execution.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:512)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:511)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:399)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollectResult(limit.scala:59)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3018)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3810)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2742)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3802)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:126)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:267)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:852)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:217)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3800)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2742)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2949)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:306)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:343)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/itemsDf.parquet/part-00007-tid-2263344001468129208-3575ab9f-2487-4d5e-bd57-2b32b9f64dfc-52-1-c000.snappy.parquet. Parquet column cannot be converted. Column: [attributes, list, element], Expected: ByteType, Found: BINARY\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:389)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:359)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$prepareNextFile$1(FileScanRDD.scala:559)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:68)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:54)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:101)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException\n\tat com.databricks.sql.io.parquet.VectorizedColumnReader.constructConvertNotSupportedException(VectorizedColumnReader.java:448)\n\tat com.databricks.sql.io.parquet.VectorizedColumnReader.readBinaryBatch(VectorizedColumnReader.java:827)\n\tat com.databricks.sql.io.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:406)\n\tat com.databricks.sql.io.parquet.VectorizedArrayReader.readBatch(VectorizedArrayReader.java:119)\n\tat com.databricks.sql.io.parquet.DatabricksVectorizedParquetRecordReader.nextBatch(DatabricksVectorizedParquetRecordReader.java:480)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:190)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:333)\n\t... 18 more\n</div>"]}}], "execution_count": 0}, {"cell_type": "markdown", "source": ["## Answer 2 (incorrect)"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "8fef6132-53bf-4c40-b876-e17972c28fe8"}}}, {"cell_type": "code", "source": ["from pyspark.sql.types import StructType, StructField, IntegerType, ArrayType, StringType\n\nitemsDfSchema = StructType([\n  StructField(\"itemId\", IntegerType),\n  StructField(\"attributes\", ArrayType(StringType)),\n  StructField(\"supplier\", StringType)])\n\nitemsDf = spark.read.schema(itemsDfSchema).parquet(filePath)\n\nitemsDf.show(truncate=False)\nitemsDf.printSchema()"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "4a1dfb7c-aaf8-4c46-89c0-7367cb77a413"}}, "outputs": [{"output_type": "display_data", "metadata": {"application/vnd.databricks.v1+output": {"datasetInfos": [], "data": "<div class=\"ansiout\"></div>", "removedWidgets": [], "addedWidgets": {}, "metadata": {}, "type": "html", "arguments": {}}}, "data": {"text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}, {"output_type": "display_data", "metadata": {"application/vnd.databricks.v1+output": {"data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AssertionError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4039916869340885&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> itemsDfSchema = StructType([\n<span class=\"ansi-green-fg\">----&gt; 4</span><span class=\"ansi-red-fg\">   </span>StructField<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;itemId&#34;</span><span class=\"ansi-blue-fg\">,</span> IntegerType<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span>   StructField<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;attributes&#34;</span><span class=\"ansi-blue-fg\">,</span> ArrayType<span class=\"ansi-blue-fg\">(</span>StringType<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span>   StructField(&#34;supplier&#34;, StringType)])\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/types.py</span> in <span class=\"ansi-cyan-fg\">__init__</span><span class=\"ansi-blue-fg\">(self, name, dataType, nullable, metadata)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    415</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    416</span>     <span class=\"ansi-green-fg\">def</span> __init__<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> name<span class=\"ansi-blue-fg\">,</span> dataType<span class=\"ansi-blue-fg\">,</span> nullable<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">,</span> metadata<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 417</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">assert</span> isinstance<span class=\"ansi-blue-fg\">(</span>dataType<span class=\"ansi-blue-fg\">,</span> DataType<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    418</span>             <span class=\"ansi-blue-fg\">&#34;dataType %s should be an instance of %s&#34;</span> <span class=\"ansi-blue-fg\">%</span> <span class=\"ansi-blue-fg\">(</span>dataType<span class=\"ansi-blue-fg\">,</span> DataType<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    419</span>         <span class=\"ansi-green-fg\">assert</span> isinstance<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">,</span> str<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;field name %s should be a string&#34;</span> <span class=\"ansi-blue-fg\">%</span> <span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AssertionError</span>: dataType &lt;class &#39;pyspark.sql.types.IntegerType&#39;&gt; should be an instance of &lt;class &#39;pyspark.sql.types.DataType&#39;&gt;</div>", "errorSummary": "<span class=\"ansi-red-fg\">AssertionError</span>: dataType &lt;class &#39;pyspark.sql.types.IntegerType&#39;&gt; should be an instance of &lt;class &#39;pyspark.sql.types.DataType&#39;&gt;", "metadata": {}, "errorTraceType": "html", "type": "ipynbError", "arguments": {}}}, "data": {"text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AssertionError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4039916869340885&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> itemsDfSchema = StructType([\n<span class=\"ansi-green-fg\">----&gt; 4</span><span class=\"ansi-red-fg\">   </span>StructField<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;itemId&#34;</span><span class=\"ansi-blue-fg\">,</span> IntegerType<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span>   StructField<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;attributes&#34;</span><span class=\"ansi-blue-fg\">,</span> ArrayType<span class=\"ansi-blue-fg\">(</span>StringType<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span>   StructField(&#34;supplier&#34;, StringType)])\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/types.py</span> in <span class=\"ansi-cyan-fg\">__init__</span><span class=\"ansi-blue-fg\">(self, name, dataType, nullable, metadata)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    415</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    416</span>     <span class=\"ansi-green-fg\">def</span> __init__<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> name<span class=\"ansi-blue-fg\">,</span> dataType<span class=\"ansi-blue-fg\">,</span> nullable<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">,</span> metadata<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 417</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">assert</span> isinstance<span class=\"ansi-blue-fg\">(</span>dataType<span class=\"ansi-blue-fg\">,</span> DataType<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    418</span>             <span class=\"ansi-blue-fg\">&#34;dataType %s should be an instance of %s&#34;</span> <span class=\"ansi-blue-fg\">%</span> <span class=\"ansi-blue-fg\">(</span>dataType<span class=\"ansi-blue-fg\">,</span> DataType<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    419</span>         <span class=\"ansi-green-fg\">assert</span> isinstance<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">,</span> str<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;field name %s should be a string&#34;</span> <span class=\"ansi-blue-fg\">%</span> <span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AssertionError</span>: dataType &lt;class &#39;pyspark.sql.types.IntegerType&#39;&gt; should be an instance of &lt;class &#39;pyspark.sql.types.DataType&#39;&gt;</div>"]}}], "execution_count": 0}, {"cell_type": "markdown", "source": ["## Answer 3 (incorrect)"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "9edca3fe-7135-4cbe-a61a-a5dbd2d4c733"}}}, {"cell_type": "code", "source": ["from pyspark.sql.types import StructType, StructField, IntegerType, ArrayType, StringType\n\nitemsDf = spark.read.schema('itemId integer, attributes <string>, supplier string').parquet(filePath)\n\nitemsDf.show(truncate=False)\nitemsDf.printSchema()"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "6c3f712f-0536-4ce8-ad81-b36c5ec71945"}}, "outputs": [{"output_type": "display_data", "metadata": {"application/vnd.databricks.v1+output": {"datasetInfos": [], "data": "<div class=\"ansiout\"></div>", "removedWidgets": [], "addedWidgets": {}, "metadata": {}, "type": "html", "arguments": {}}}, "data": {"text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}, {"output_type": "display_data", "metadata": {"application/vnd.databricks.v1+output": {"data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ParseException</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4039916869340887&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>types <span class=\"ansi-green-fg\">import</span> StructType<span class=\"ansi-blue-fg\">,</span> StructField<span class=\"ansi-blue-fg\">,</span> IntegerType<span class=\"ansi-blue-fg\">,</span> ArrayType<span class=\"ansi-blue-fg\">,</span> StringType\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>itemsDf <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>schema<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;itemId integer, attributes &lt;string&gt;, supplier string&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>parquet<span class=\"ansi-blue-fg\">(</span>filePath<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> itemsDf<span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">False</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">schema</span><span class=\"ansi-blue-fg\">(self, schema)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    101</span>             self<span class=\"ansi-blue-fg\">.</span>_jreader <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jreader<span class=\"ansi-blue-fg\">.</span>schema<span class=\"ansi-blue-fg\">(</span>jschema<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    102</span>         <span class=\"ansi-green-fg\">elif</span> isinstance<span class=\"ansi-blue-fg\">(</span>schema<span class=\"ansi-blue-fg\">,</span> str<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 103</span><span class=\"ansi-red-fg\">             </span>self<span class=\"ansi-blue-fg\">.</span>_jreader <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jreader<span class=\"ansi-blue-fg\">.</span>schema<span class=\"ansi-blue-fg\">(</span>schema<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    104</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    105</span>             <span class=\"ansi-green-fg\">raise</span> TypeError<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;schema should be StructType or string&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    121</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    122</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n<span class=\"ansi-green-fg\">--&gt; 123</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> converted <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    124</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>                 <span class=\"ansi-green-fg\">raise</span>\n\n<span class=\"ansi-red-fg\">ParseException</span>: \nextraneous input &#39;&lt;&#39; expecting {&#39;CALLED&#39;, &#39;CLONE&#39;, &#39;COLLECT&#39;, &#39;CONTAINS&#39;, &#39;CONVERT&#39;, &#39;COPY&#39;, &#39;COPY_OPTIONS&#39;, &#39;CREDENTIALS&#39;, &#39;DEEP&#39;, &#39;DEFINER&#39;, &#39;DELTA&#39;, &#39;DETERMINISTIC&#39;, &#39;ENCRYPTION&#39;, &#39;EXPECT&#39;, &#39;FAIL&#39;, &#39;FILES&#39;, &#39;FORMAT_OPTIONS&#39;, &#39;HISTORY&#39;, &#39;INCREMENTAL&#39;, &#39;INPUT&#39;, &#39;INVOKER&#39;, &#39;LANGUAGE&#39;, &#39;LIVE&#39;, &#39;MODIFIES&#39;, &#39;OPTIMIZE&#39;, &#39;PATTERN&#39;, &#39;READS&#39;, &#39;RESTORE&#39;, &#39;RETURN&#39;, &#39;RETURNS&#39;, &#39;SAMPLE&#39;, &#39;SECURITY&#39;, &#39;SHALLOW&#39;, &#39;SPECIFIC&#39;, &#39;SQL&#39;, &#39;TIMESTAMP&#39;, &#39;VERSION&#39;, &#39;VIOLATION&#39;, &#39;ZORDER&#39;, &#39;ADD&#39;, &#39;AFTER&#39;, &#39;ALL&#39;, &#39;ALTER&#39;, &#39;ALWAYS&#39;, &#39;ANALYZE&#39;, &#39;AND&#39;, &#39;ANTI&#39;, &#39;ANY&#39;, &#39;ARCHIVE&#39;, &#39;ARRAY&#39;, &#39;AS&#39;, &#39;ASC&#39;, &#39;AT&#39;, &#39;AUTHORIZATION&#39;, &#39;BETWEEN&#39;, &#39;BOTH&#39;, &#39;BUCKET&#39;, &#39;BUCKETS&#39;, &#39;BY&#39;, &#39;CACHE&#39;, &#39;CASCADE&#39;, &#39;CASE&#39;, &#39;CAST&#39;, &#39;CHANGE&#39;, &#39;CHECK&#39;, &#39;CLEAR&#39;, &#39;CLUSTER&#39;, &#39;CLUSTERED&#39;, &#39;CODEGEN&#39;, &#39;COLLATE&#39;, &#39;COLLECTION&#39;, &#39;COLUMN&#39;, &#39;COLUMNS&#39;, &#39;COMMENT&#39;, &#39;COMMIT&#39;, &#39;COMPACT&#39;, &#39;COMPACTIONS&#39;, &#39;COMPUTE&#39;, &#39;CONCATENATE&#39;, &#39;CONSTRAINT&#39;, &#39;COST&#39;, &#39;CREATE&#39;, &#39;CROSS&#39;, &#39;CUBE&#39;, &#39;CURRENT&#39;, &#39;CURRENT_DATE&#39;, &#39;CURRENT_TIME&#39;, &#39;CURRENT_TIMESTAMP&#39;, &#39;CURRENT_USER&#39;, &#39;DATA&#39;, &#39;DATABASE&#39;, DATABASES, &#39;DBPROPERTIES&#39;, &#39;DEFINED&#39;, &#39;DELETE&#39;, &#39;DELIMITED&#39;, &#39;DESC&#39;, &#39;DESCRIBE&#39;, &#39;DFS&#39;, &#39;DIRECTORIES&#39;, &#39;DIRECTORY&#39;, &#39;DISTINCT&#39;, &#39;DISTRIBUTE&#39;, &#39;DIV&#39;, &#39;DROP&#39;, &#39;ELSE&#39;, &#39;END&#39;, &#39;ESCAPE&#39;, &#39;ESCAPED&#39;, &#39;EXCEPT&#39;, &#39;EXCHANGE&#39;, &#39;EXISTS&#39;, &#39;EXPLAIN&#39;, &#39;EXPORT&#39;, &#39;EXTENDED&#39;, &#39;EXTERNAL&#39;, &#39;EXTRACT&#39;, &#39;FALSE&#39;, &#39;FETCH&#39;, &#39;FIELDS&#39;, &#39;FILTER&#39;, &#39;FILEFORMAT&#39;, &#39;FIRST&#39;, &#39;FN&#39;, &#39;FOLLOWING&#39;, &#39;FOR&#39;, &#39;FOREIGN&#39;, &#39;FORMAT&#39;, &#39;FORMATTED&#39;, &#39;FROM&#39;, &#39;FULL&#39;, &#39;FUNCTION&#39;, &#39;FUNCTIONS&#39;, &#39;GENERATED&#39;, &#39;GLOBAL&#39;, &#39;GRANT&#39;, &#39;GROUP&#39;, &#39;GROUPING&#39;, &#39;HAVING&#39;, &#39;IF&#39;, &#39;IGNORE&#39;, &#39;IMPORT&#39;, &#39;IN&#39;, &#39;INDEX&#39;, &#39;INDEXES&#39;, &#39;INNER&#39;, &#39;INPATH&#39;, &#39;INPUTFORMAT&#39;, &#39;INSERT&#39;, &#39;INTERSECT&#39;, &#39;INTERVAL&#39;, &#39;INTO&#39;, &#39;IS&#39;, &#39;ITEMS&#39;, &#39;JOIN&#39;, &#39;KEY&#39;, &#39;KEYS&#39;, &#39;LAST&#39;, &#39;LATERAL&#39;, &#39;LAZY&#39;, &#39;LEADING&#39;, &#39;LEFT&#39;, &#39;LIKE&#39;, &#39;LIMIT&#39;, &#39;LINES&#39;, &#39;LIST&#39;, &#39;LOAD&#39;, &#39;LOCAL&#39;, &#39;LOCATION&#39;, &#39;LOCK&#39;, &#39;LOCKS&#39;, &#39;LOGICAL&#39;, &#39;MACRO&#39;, &#39;MAP&#39;, &#39;MATCHED&#39;, &#39;MERGE&#39;, &#39;MSCK&#39;, &#39;NAMESPACE&#39;, &#39;NAMESPACES&#39;, &#39;NATURAL&#39;, &#39;NO&#39;, NOT, &#39;NULL&#39;, &#39;NULLS&#39;, &#39;OF&#39;, &#39;ON&#39;, &#39;ONLY&#39;, &#39;OPTION&#39;, &#39;OPTIONS&#39;, &#39;OR&#39;, &#39;ORDER&#39;, &#39;OUT&#39;, &#39;OUTER&#39;, &#39;OUTPUTFORMAT&#39;, &#39;OVER&#39;, &#39;OVERLAPS&#39;, &#39;OVERLAY&#39;, &#39;OVERWRITE&#39;, &#39;PARTITION&#39;, &#39;PARTITIONED&#39;, &#39;PARTITIONS&#39;, &#39;PERCENT&#39;, &#39;PIVOT&#39;, &#39;PLACING&#39;, &#39;POSITION&#39;, &#39;PRECEDING&#39;, &#39;PRIMARY&#39;, &#39;PRINCIPALS&#39;, &#39;PROPERTIES&#39;, &#39;PURGE&#39;, &#39;QUERY&#39;, &#39;RANGE&#39;, &#39;RECORDREADER&#39;, &#39;RECORDWRITER&#39;, &#39;RECOVER&#39;, &#39;REDUCE&#39;, &#39;REFERENCES&#39;, &#39;REFRESH&#39;, &#39;RENAME&#39;, &#39;REPAIR&#39;, &#39;REPLACE&#39;, &#39;RESET&#39;, &#39;RESTRICT&#39;, &#39;REVOKE&#39;, &#39;RIGHT&#39;, RLIKE, &#39;ROLE&#39;, &#39;ROLES&#39;, &#39;ROLLBACK&#39;, &#39;ROLLUP&#39;, &#39;ROW&#39;, &#39;ROWS&#39;, &#39;SCHEMA&#39;, &#39;SELECT&#39;, &#39;SEMI&#39;, &#39;SEPARATED&#39;, &#39;SERDE&#39;, &#39;SERDEPROPERTIES&#39;, &#39;SESSION_USER&#39;, &#39;SET&#39;, &#39;MINUS&#39;, &#39;SETS&#39;, &#39;SHOW&#39;, &#39;SKEWED&#39;, &#39;SOME&#39;, &#39;SORT&#39;, &#39;SORTED&#39;, &#39;START&#39;, &#39;STATISTICS&#39;, &#39;STORED&#39;, &#39;STRATIFY&#39;, &#39;STRUCT&#39;, &#39;SUBSTR&#39;, &#39;SUBSTRING&#39;, &#39;TABLE&#39;, &#39;TABLES&#39;, &#39;TABLESAMPLE&#39;, &#39;TBLPROPERTIES&#39;, TEMPORARY, &#39;TERMINATED&#39;, &#39;THEN&#39;, &#39;TIME&#39;, &#39;TO&#39;, &#39;TOUCH&#39;, &#39;TRAILING&#39;, &#39;TRANSACTION&#39;, &#39;TRANSACTIONS&#39;, &#39;TRANSFORM&#39;, &#39;TRIM&#39;, &#39;TRUE&#39;, &#39;TRUNCATE&#39;, &#39;TRY_CAST&#39;, &#39;TYPE&#39;, &#39;UNARCHIVE&#39;, &#39;UNBOUNDED&#39;, &#39;UNCACHE&#39;, &#39;UNION&#39;, &#39;UNIQUE&#39;, &#39;UNKNOWN&#39;, &#39;UNLOCK&#39;, &#39;UNSET&#39;, &#39;UPDATE&#39;, &#39;USE&#39;, &#39;USER&#39;, &#39;USING&#39;, &#39;VALUES&#39;, &#39;VIEW&#39;, &#39;VIEWS&#39;, &#39;WHEN&#39;, &#39;WHERE&#39;, &#39;WINDOW&#39;, &#39;WITH&#39;, &#39;ZONE&#39;, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 27)\n\n== SQL ==\nitemId integer, attributes &lt;string&gt;, supplier string\n---------------------------^^^\n</div>", "errorSummary": "<span class=\"ansi-red-fg\">ParseException</span>: ", "metadata": {}, "errorTraceType": "html", "type": "ipynbError", "arguments": {}}}, "data": {"text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ParseException</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4039916869340887&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>types <span class=\"ansi-green-fg\">import</span> StructType<span class=\"ansi-blue-fg\">,</span> StructField<span class=\"ansi-blue-fg\">,</span> IntegerType<span class=\"ansi-blue-fg\">,</span> ArrayType<span class=\"ansi-blue-fg\">,</span> StringType\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>itemsDf <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>schema<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;itemId integer, attributes &lt;string&gt;, supplier string&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>parquet<span class=\"ansi-blue-fg\">(</span>filePath<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> itemsDf<span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">False</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">schema</span><span class=\"ansi-blue-fg\">(self, schema)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    101</span>             self<span class=\"ansi-blue-fg\">.</span>_jreader <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jreader<span class=\"ansi-blue-fg\">.</span>schema<span class=\"ansi-blue-fg\">(</span>jschema<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    102</span>         <span class=\"ansi-green-fg\">elif</span> isinstance<span class=\"ansi-blue-fg\">(</span>schema<span class=\"ansi-blue-fg\">,</span> str<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 103</span><span class=\"ansi-red-fg\">             </span>self<span class=\"ansi-blue-fg\">.</span>_jreader <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jreader<span class=\"ansi-blue-fg\">.</span>schema<span class=\"ansi-blue-fg\">(</span>schema<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    104</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    105</span>             <span class=\"ansi-green-fg\">raise</span> TypeError<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;schema should be StructType or string&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    121</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    122</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n<span class=\"ansi-green-fg\">--&gt; 123</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> converted <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    124</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>                 <span class=\"ansi-green-fg\">raise</span>\n\n<span class=\"ansi-red-fg\">ParseException</span>: \nextraneous input &#39;&lt;&#39; expecting {&#39;CALLED&#39;, &#39;CLONE&#39;, &#39;COLLECT&#39;, &#39;CONTAINS&#39;, &#39;CONVERT&#39;, &#39;COPY&#39;, &#39;COPY_OPTIONS&#39;, &#39;CREDENTIALS&#39;, &#39;DEEP&#39;, &#39;DEFINER&#39;, &#39;DELTA&#39;, &#39;DETERMINISTIC&#39;, &#39;ENCRYPTION&#39;, &#39;EXPECT&#39;, &#39;FAIL&#39;, &#39;FILES&#39;, &#39;FORMAT_OPTIONS&#39;, &#39;HISTORY&#39;, &#39;INCREMENTAL&#39;, &#39;INPUT&#39;, &#39;INVOKER&#39;, &#39;LANGUAGE&#39;, &#39;LIVE&#39;, &#39;MODIFIES&#39;, &#39;OPTIMIZE&#39;, &#39;PATTERN&#39;, &#39;READS&#39;, &#39;RESTORE&#39;, &#39;RETURN&#39;, &#39;RETURNS&#39;, &#39;SAMPLE&#39;, &#39;SECURITY&#39;, &#39;SHALLOW&#39;, &#39;SPECIFIC&#39;, &#39;SQL&#39;, &#39;TIMESTAMP&#39;, &#39;VERSION&#39;, &#39;VIOLATION&#39;, &#39;ZORDER&#39;, &#39;ADD&#39;, &#39;AFTER&#39;, &#39;ALL&#39;, &#39;ALTER&#39;, &#39;ALWAYS&#39;, &#39;ANALYZE&#39;, &#39;AND&#39;, &#39;ANTI&#39;, &#39;ANY&#39;, &#39;ARCHIVE&#39;, &#39;ARRAY&#39;, &#39;AS&#39;, &#39;ASC&#39;, &#39;AT&#39;, &#39;AUTHORIZATION&#39;, &#39;BETWEEN&#39;, &#39;BOTH&#39;, &#39;BUCKET&#39;, &#39;BUCKETS&#39;, &#39;BY&#39;, &#39;CACHE&#39;, &#39;CASCADE&#39;, &#39;CASE&#39;, &#39;CAST&#39;, &#39;CHANGE&#39;, &#39;CHECK&#39;, &#39;CLEAR&#39;, &#39;CLUSTER&#39;, &#39;CLUSTERED&#39;, &#39;CODEGEN&#39;, &#39;COLLATE&#39;, &#39;COLLECTION&#39;, &#39;COLUMN&#39;, &#39;COLUMNS&#39;, &#39;COMMENT&#39;, &#39;COMMIT&#39;, &#39;COMPACT&#39;, &#39;COMPACTIONS&#39;, &#39;COMPUTE&#39;, &#39;CONCATENATE&#39;, &#39;CONSTRAINT&#39;, &#39;COST&#39;, &#39;CREATE&#39;, &#39;CROSS&#39;, &#39;CUBE&#39;, &#39;CURRENT&#39;, &#39;CURRENT_DATE&#39;, &#39;CURRENT_TIME&#39;, &#39;CURRENT_TIMESTAMP&#39;, &#39;CURRENT_USER&#39;, &#39;DATA&#39;, &#39;DATABASE&#39;, DATABASES, &#39;DBPROPERTIES&#39;, &#39;DEFINED&#39;, &#39;DELETE&#39;, &#39;DELIMITED&#39;, &#39;DESC&#39;, &#39;DESCRIBE&#39;, &#39;DFS&#39;, &#39;DIRECTORIES&#39;, &#39;DIRECTORY&#39;, &#39;DISTINCT&#39;, &#39;DISTRIBUTE&#39;, &#39;DIV&#39;, &#39;DROP&#39;, &#39;ELSE&#39;, &#39;END&#39;, &#39;ESCAPE&#39;, &#39;ESCAPED&#39;, &#39;EXCEPT&#39;, &#39;EXCHANGE&#39;, &#39;EXISTS&#39;, &#39;EXPLAIN&#39;, &#39;EXPORT&#39;, &#39;EXTENDED&#39;, &#39;EXTERNAL&#39;, &#39;EXTRACT&#39;, &#39;FALSE&#39;, &#39;FETCH&#39;, &#39;FIELDS&#39;, &#39;FILTER&#39;, &#39;FILEFORMAT&#39;, &#39;FIRST&#39;, &#39;FN&#39;, &#39;FOLLOWING&#39;, &#39;FOR&#39;, &#39;FOREIGN&#39;, &#39;FORMAT&#39;, &#39;FORMATTED&#39;, &#39;FROM&#39;, &#39;FULL&#39;, &#39;FUNCTION&#39;, &#39;FUNCTIONS&#39;, &#39;GENERATED&#39;, &#39;GLOBAL&#39;, &#39;GRANT&#39;, &#39;GROUP&#39;, &#39;GROUPING&#39;, &#39;HAVING&#39;, &#39;IF&#39;, &#39;IGNORE&#39;, &#39;IMPORT&#39;, &#39;IN&#39;, &#39;INDEX&#39;, &#39;INDEXES&#39;, &#39;INNER&#39;, &#39;INPATH&#39;, &#39;INPUTFORMAT&#39;, &#39;INSERT&#39;, &#39;INTERSECT&#39;, &#39;INTERVAL&#39;, &#39;INTO&#39;, &#39;IS&#39;, &#39;ITEMS&#39;, &#39;JOIN&#39;, &#39;KEY&#39;, &#39;KEYS&#39;, &#39;LAST&#39;, &#39;LATERAL&#39;, &#39;LAZY&#39;, &#39;LEADING&#39;, &#39;LEFT&#39;, &#39;LIKE&#39;, &#39;LIMIT&#39;, &#39;LINES&#39;, &#39;LIST&#39;, &#39;LOAD&#39;, &#39;LOCAL&#39;, &#39;LOCATION&#39;, &#39;LOCK&#39;, &#39;LOCKS&#39;, &#39;LOGICAL&#39;, &#39;MACRO&#39;, &#39;MAP&#39;, &#39;MATCHED&#39;, &#39;MERGE&#39;, &#39;MSCK&#39;, &#39;NAMESPACE&#39;, &#39;NAMESPACES&#39;, &#39;NATURAL&#39;, &#39;NO&#39;, NOT, &#39;NULL&#39;, &#39;NULLS&#39;, &#39;OF&#39;, &#39;ON&#39;, &#39;ONLY&#39;, &#39;OPTION&#39;, &#39;OPTIONS&#39;, &#39;OR&#39;, &#39;ORDER&#39;, &#39;OUT&#39;, &#39;OUTER&#39;, &#39;OUTPUTFORMAT&#39;, &#39;OVER&#39;, &#39;OVERLAPS&#39;, &#39;OVERLAY&#39;, &#39;OVERWRITE&#39;, &#39;PARTITION&#39;, &#39;PARTITIONED&#39;, &#39;PARTITIONS&#39;, &#39;PERCENT&#39;, &#39;PIVOT&#39;, &#39;PLACING&#39;, &#39;POSITION&#39;, &#39;PRECEDING&#39;, &#39;PRIMARY&#39;, &#39;PRINCIPALS&#39;, &#39;PROPERTIES&#39;, &#39;PURGE&#39;, &#39;QUERY&#39;, &#39;RANGE&#39;, &#39;RECORDREADER&#39;, &#39;RECORDWRITER&#39;, &#39;RECOVER&#39;, &#39;REDUCE&#39;, &#39;REFERENCES&#39;, &#39;REFRESH&#39;, &#39;RENAME&#39;, &#39;REPAIR&#39;, &#39;REPLACE&#39;, &#39;RESET&#39;, &#39;RESTRICT&#39;, &#39;REVOKE&#39;, &#39;RIGHT&#39;, RLIKE, &#39;ROLE&#39;, &#39;ROLES&#39;, &#39;ROLLBACK&#39;, &#39;ROLLUP&#39;, &#39;ROW&#39;, &#39;ROWS&#39;, &#39;SCHEMA&#39;, &#39;SELECT&#39;, &#39;SEMI&#39;, &#39;SEPARATED&#39;, &#39;SERDE&#39;, &#39;SERDEPROPERTIES&#39;, &#39;SESSION_USER&#39;, &#39;SET&#39;, &#39;MINUS&#39;, &#39;SETS&#39;, &#39;SHOW&#39;, &#39;SKEWED&#39;, &#39;SOME&#39;, &#39;SORT&#39;, &#39;SORTED&#39;, &#39;START&#39;, &#39;STATISTICS&#39;, &#39;STORED&#39;, &#39;STRATIFY&#39;, &#39;STRUCT&#39;, &#39;SUBSTR&#39;, &#39;SUBSTRING&#39;, &#39;TABLE&#39;, &#39;TABLES&#39;, &#39;TABLESAMPLE&#39;, &#39;TBLPROPERTIES&#39;, TEMPORARY, &#39;TERMINATED&#39;, &#39;THEN&#39;, &#39;TIME&#39;, &#39;TO&#39;, &#39;TOUCH&#39;, &#39;TRAILING&#39;, &#39;TRANSACTION&#39;, &#39;TRANSACTIONS&#39;, &#39;TRANSFORM&#39;, &#39;TRIM&#39;, &#39;TRUE&#39;, &#39;TRUNCATE&#39;, &#39;TRY_CAST&#39;, &#39;TYPE&#39;, &#39;UNARCHIVE&#39;, &#39;UNBOUNDED&#39;, &#39;UNCACHE&#39;, &#39;UNION&#39;, &#39;UNIQUE&#39;, &#39;UNKNOWN&#39;, &#39;UNLOCK&#39;, &#39;UNSET&#39;, &#39;UPDATE&#39;, &#39;USE&#39;, &#39;USER&#39;, &#39;USING&#39;, &#39;VALUES&#39;, &#39;VIEW&#39;, &#39;VIEWS&#39;, &#39;WHEN&#39;, &#39;WHERE&#39;, &#39;WINDOW&#39;, &#39;WITH&#39;, &#39;ZONE&#39;, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 27)\n\n== SQL ==\nitemId integer, attributes &lt;string&gt;, supplier string\n---------------------------^^^\n</div>"]}}], "execution_count": 0}, {"cell_type": "markdown", "source": ["## Answer 4 (correct)"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "2f909ada-99cb-406f-8df0-0f31d52f554c"}}}, {"cell_type": "code", "source": ["from pyspark.sql.types import StructType, StructField, IntegerType, ArrayType, StringType\n\nitemsDfSchema = StructType([\n  StructField(\"itemId\", IntegerType()),\n  StructField(\"attributes\", ArrayType(StringType())),\n  StructField(\"supplier\", StringType())])\n\nitemsDf = spark.read.schema(itemsDfSchema).parquet(filePath)\n\nitemsDf.show(truncate=False)\nitemsDf.printSchema()"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "c7193aa7-5004-476d-a11a-15dd3833c449"}}, "outputs": [{"output_type": "display_data", "metadata": {"application/vnd.databricks.v1+output": {"datasetInfos": [], "data": "<div class=\"ansiout\">+------+-----------------------------+-------------------+\n|itemId|attributes                   |supplier           |\n+------+-----------------------------+-------------------+\n|3     |[green, summer, travel]      |Sports Company Inc.|\n|1     |[blue, winter, cozy]         |Sports Company Inc.|\n|2     |[red, summer, fresh, cooling]|YetiX              |\n+------+-----------------------------+-------------------+\n\nroot\n |-- itemId: integer (nullable = true)\n |-- attributes: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- supplier: string (nullable = true)\n\n</div>", "removedWidgets": [], "addedWidgets": {}, "metadata": {}, "type": "html", "arguments": {}}}, "data": {"text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+-----------------------------+-------------------+\nitemId|attributes                   |supplier           |\n+------+-----------------------------+-------------------+\n3     |[green, summer, travel]      |Sports Company Inc.|\n1     |[blue, winter, cozy]         |Sports Company Inc.|\n2     |[red, summer, fresh, cooling]|YetiX              |\n+------+-----------------------------+-------------------+\n\nroot\n-- itemId: integer (nullable = true)\n-- attributes: array (nullable = true)\n    |-- element: string (containsNull = true)\n-- supplier: string (nullable = true)\n\n</div>"]}}], "execution_count": 0}, {"cell_type": "markdown", "source": ["## Answer 5 (incorrect)"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "3da08153-8385-45d8-85f4-9d1e5156ec37"}}}, {"cell_type": "code", "source": ["from pyspark.sql.types import StructType, StructField, IntegerType, ArrayType, StringType\n\nitemsDfSchema = StructType([\n  StructField(\"itemId\", IntegerType()),\n  StructField(\"attributes\", ArrayType([StringType()])),\n  StructField(\"supplier\", StringType())])\n\nitemsDf = spark.read(schema=itemsDfSchema).parquet(filePath)\n\nitemsDf.show(truncate=False)\nitemsDf.printSchema()"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "741846ba-74a7-4878-9715-49e613b7b7e2"}}, "outputs": [{"output_type": "display_data", "metadata": {"application/vnd.databricks.v1+output": {"datasetInfos": [], "data": "<div class=\"ansiout\"></div>", "removedWidgets": [], "addedWidgets": {}, "metadata": {}, "type": "html", "arguments": {}}}, "data": {"text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}, {"output_type": "display_data", "metadata": {"application/vnd.databricks.v1+output": {"data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AssertionError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4039916869340889&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> itemsDfSchema = StructType([\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span>   StructField<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;itemId&#34;</span><span class=\"ansi-blue-fg\">,</span> IntegerType<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\">   </span>StructField<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;attributes&#34;</span><span class=\"ansi-blue-fg\">,</span> ArrayType<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">[</span>StringType<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span>   StructField(&#34;supplier&#34;, StringType())])\n<span class=\"ansi-green-intense-fg ansi-bold\">      7</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/types.py</span> in <span class=\"ansi-cyan-fg\">__init__</span><span class=\"ansi-blue-fg\">(self, elementType, containsNull)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    284</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    285</span>     <span class=\"ansi-green-fg\">def</span> __init__<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> elementType<span class=\"ansi-blue-fg\">,</span> containsNull<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 286</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">assert</span> isinstance<span class=\"ansi-blue-fg\">(</span>elementType<span class=\"ansi-blue-fg\">,</span> DataType<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    287</span>             <span class=\"ansi-blue-fg\">&#34;elementType %s should be an instance of %s&#34;</span> <span class=\"ansi-blue-fg\">%</span> <span class=\"ansi-blue-fg\">(</span>elementType<span class=\"ansi-blue-fg\">,</span> DataType<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    288</span>         self<span class=\"ansi-blue-fg\">.</span>elementType <span class=\"ansi-blue-fg\">=</span> elementType\n\n<span class=\"ansi-red-fg\">AssertionError</span>: elementType [StringType] should be an instance of &lt;class &#39;pyspark.sql.types.DataType&#39;&gt;</div>", "errorSummary": "<span class=\"ansi-red-fg\">AssertionError</span>: elementType [StringType] should be an instance of &lt;class &#39;pyspark.sql.types.DataType&#39;&gt;", "metadata": {}, "errorTraceType": "html", "type": "ipynbError", "arguments": {}}}, "data": {"text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AssertionError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4039916869340889&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> itemsDfSchema = StructType([\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span>   StructField<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;itemId&#34;</span><span class=\"ansi-blue-fg\">,</span> IntegerType<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\">   </span>StructField<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;attributes&#34;</span><span class=\"ansi-blue-fg\">,</span> ArrayType<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">[</span>StringType<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span>   StructField(&#34;supplier&#34;, StringType())])\n<span class=\"ansi-green-intense-fg ansi-bold\">      7</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/types.py</span> in <span class=\"ansi-cyan-fg\">__init__</span><span class=\"ansi-blue-fg\">(self, elementType, containsNull)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    284</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    285</span>     <span class=\"ansi-green-fg\">def</span> __init__<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> elementType<span class=\"ansi-blue-fg\">,</span> containsNull<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 286</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">assert</span> isinstance<span class=\"ansi-blue-fg\">(</span>elementType<span class=\"ansi-blue-fg\">,</span> DataType<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    287</span>             <span class=\"ansi-blue-fg\">&#34;elementType %s should be an instance of %s&#34;</span> <span class=\"ansi-blue-fg\">%</span> <span class=\"ansi-blue-fg\">(</span>elementType<span class=\"ansi-blue-fg\">,</span> DataType<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    288</span>         self<span class=\"ansi-blue-fg\">.</span>elementType <span class=\"ansi-blue-fg\">=</span> elementType\n\n<span class=\"ansi-red-fg\">AssertionError</span>: elementType [StringType] should be an instance of &lt;class &#39;pyspark.sql.types.DataType&#39;&gt;</div>"]}}], "execution_count": 0}], "metadata": {"application/vnd.databricks.v1+notebook": {"notebookName": "139", "dashboards": [], "notebookMetadata": {"pythonIndentUnit": 2}, "language": "python", "widgets": {}, "notebookOrigID": 4039916869340877}}, "nbformat": 4, "nbformat_minor": 0}