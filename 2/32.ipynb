{"cells": [{"cell_type": "markdown", "source": ["# Test 2, Question 32\n> **Hint:** In Databricks, import code for all questions via this URL:\n> \n> https://github.com/flrs/spark_practice_tests_code/raw/main/spark_practice_tests_code.dbc"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "87a4339f-cc26-4446-83b0-5fd419c20c70"}}}, {"cell_type": "code", "source": ["%run ./create_itemsDf"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "ce0bf962-8af0-4c63-8bed-7acbfb428c56"}}, "outputs": [{"output_type": "display_data", "metadata": {"application/vnd.databricks.v1+output": {"datasetInfos": [], "data": "<div class=\"ansiout\"></div>", "removedWidgets": [], "addedWidgets": {}, "metadata": {}, "type": "html", "arguments": {}}}, "data": {"text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}], "execution_count": 0}, {"cell_type": "markdown", "source": ["## Answer 1 (incorrect)"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "3da08153-8385-45d8-85f4-9d1e5156ec37"}}}, {"cell_type": "code", "source": ["from pyspark.sql.functions import sort, col\n\nitemsDf.withColumn('attributes', sort(col('attributes'), asc=False)).show()"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "741846ba-74a7-4878-9715-49e613b7b7e2"}}, "outputs": [{"output_type": "display_data", "metadata": {"application/vnd.databricks.v1+output": {"datasetInfos": [], "data": "<div class=\"ansiout\"></div>", "removedWidgets": [], "addedWidgets": {}, "metadata": {}, "type": "html", "arguments": {}}}, "data": {"text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}, {"output_type": "display_data", "metadata": {"application/vnd.databricks.v1+output": {"data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ImportError</span>                               Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1184490609996898&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>functions <span class=\"ansi-green-fg\">import</span> sort<span class=\"ansi-blue-fg\">,</span> col\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> itemsDf<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;attributes&#39;</span><span class=\"ansi-blue-fg\">,</span> sort<span class=\"ansi-blue-fg\">(</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;attributes&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> asc<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">False</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">ImportError</span>: cannot import name &#39;sort&#39; from &#39;pyspark.sql.functions&#39; (/databricks/spark/python/pyspark/sql/functions.py)</div>", "errorSummary": "<span class=\"ansi-red-fg\">ImportError</span>: cannot import name &#39;sort&#39; from &#39;pyspark.sql.functions&#39; (/databricks/spark/python/pyspark/sql/functions.py)", "metadata": {}, "errorTraceType": "html", "type": "ipynbError", "arguments": {}}}, "data": {"text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ImportError</span>                               Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1184490609996898&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>functions <span class=\"ansi-green-fg\">import</span> sort<span class=\"ansi-blue-fg\">,</span> col\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> itemsDf<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;attributes&#39;</span><span class=\"ansi-blue-fg\">,</span> sort<span class=\"ansi-blue-fg\">(</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;attributes&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> asc<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">False</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">ImportError</span>: cannot import name &#39;sort&#39; from &#39;pyspark.sql.functions&#39; (/databricks/spark/python/pyspark/sql/functions.py)</div>"]}}], "execution_count": 0}, {"cell_type": "markdown", "source": ["## Answer 2 (incorrect)"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "9e7b8a51-bd8b-45ce-aed6-488cc351332a"}}}, {"cell_type": "code", "source": ["from pyspark.sql.functions import sort_array\n\nitemsDf.select(sort_array(\"attributes\")).show()"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "14f9adb3-dc1d-4e18-8cfa-4fddecdfdfcd"}}, "outputs": [{"output_type": "display_data", "metadata": {"application/vnd.databricks.v1+output": {"datasetInfos": [], "data": "<div class=\"ansiout\">+----------------------------+\n|sort_array(attributes, true)|\n+----------------------------+\n|        [blue, cozy, winter]|\n|        [cooling, fresh, ...|\n|        [green, summer, t...|\n+----------------------------+\n\n</div>", "removedWidgets": [], "addedWidgets": {}, "metadata": {}, "type": "html", "arguments": {}}}, "data": {"text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------------------+\nsort_array(attributes, true)|\n+----------------------------+\n        [blue, cozy, winter]|\n        [cooling, fresh, ...|\n        [green, summer, t...|\n+----------------------------+\n\n</div>"]}}], "execution_count": 0}, {"cell_type": "markdown", "source": ["## Answer 3 (correct)"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "2f909ada-99cb-406f-8df0-0f31d52f554c"}}}, {"cell_type": "code", "source": ["from pyspark.sql.functions import sort_array\n\nitemsDf.withColumn(\"attributes\", sort_array(\"attributes\", asc=False)).show()"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "c7193aa7-5004-476d-a11a-15dd3833c449"}}, "outputs": [{"output_type": "display_data", "metadata": {"application/vnd.databricks.v1+output": {"datasetInfos": [], "data": "<div class=\"ansiout\">+------+--------------------+-------------------+\n|itemId|          attributes|           supplier|\n+------+--------------------+-------------------+\n|     1|[winter, cozy, blue]|Sports Company Inc.|\n|     2|[summer, red, fre...|              YetiX|\n|     3|[travel, summer, ...|Sports Company Inc.|\n+------+--------------------+-------------------+\n\n</div>", "removedWidgets": [], "addedWidgets": {}, "metadata": {}, "type": "html", "arguments": {}}}, "data": {"text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+--------------------+-------------------+\nitemId|          attributes|           supplier|\n+------+--------------------+-------------------+\n     1|[winter, cozy, blue]|Sports Company Inc.|\n     2|[summer, red, fre...|              YetiX|\n     3|[travel, summer, ...|Sports Company Inc.|\n+------+--------------------+-------------------+\n\n</div>"]}}], "execution_count": 0}, {"cell_type": "markdown", "source": ["## Answer 4 (incorrect)"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "9edca3fe-7135-4cbe-a61a-a5dbd2d4c733"}}}, {"cell_type": "code", "source": ["from pyspark.sql.functions import sort_array, col\n\nitemsDf.withColumn('attributes', sort_array(col('attributes').desc())).show()"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "6c3f712f-0536-4ce8-ad81-b36c5ec71945"}}, "outputs": [{"output_type": "display_data", "metadata": {"application/vnd.databricks.v1+output": {"datasetInfos": [], "data": "<div class=\"ansiout\"></div>", "removedWidgets": [], "addedWidgets": {}, "metadata": {}, "type": "html", "arguments": {}}}, "data": {"text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}, {"output_type": "display_data", "metadata": {"application/vnd.databricks.v1+output": {"data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1184490609996896&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>functions <span class=\"ansi-green-fg\">import</span> sort_array<span class=\"ansi-blue-fg\">,</span> col\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>itemsDf<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;attributes&#39;</span><span class=\"ansi-blue-fg\">,</span> sort_array<span class=\"ansi-blue-fg\">(</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;attributes&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>desc<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">show</span><span class=\"ansi-blue-fg\">(self, n, truncate, vertical)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    488</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    489</span>         <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">,</span> bool<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">and</span> truncate<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 490</span><span class=\"ansi-red-fg\">             </span>print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">20</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    491</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    492</span>             print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> int<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 117</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o368.showString.\n: java.lang.UnsupportedOperationException: Cannot generate code for expression: input[1, array&lt;string&gt;, true] DESC NULLS LAST\n\tat org.apache.spark.sql.catalyst.expressions.Unevaluable.doGenCode(Expression.scala:365)\n\tat org.apache.spark.sql.catalyst.expressions.Unevaluable.doGenCode$(Expression.scala:364)\n\tat org.apache.spark.sql.catalyst.expressions.SortOrder.doGenCode(SortOrder.scala:62)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCodeInternal(Expression.scala:181)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$2(Expression.scala:160)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:160)\n\tat org.apache.spark.sql.catalyst.expressions.BinaryExpression.nullSafeCodeGen(Expression.scala:666)\n\tat org.apache.spark.sql.catalyst.expressions.SortArray.doGenCode(collectionOperations.scala:937)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCodeInternal(Expression.scala:181)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.$anonfun$genCodeWithFallback$2(EdgeExpressionCodegen.scala:248)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.$anonfun$genCodeWithFallback$1(EdgeExpressionCodegen.scala:248)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.genCodeWithFallback(EdgeExpressionCodegen.scala:246)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.doGenCode(EdgeExpressionCodegen.scala:1454)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.genCode(EdgeExpressionCodegen.scala:193)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:153)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.genCodeInternal(namedExpressions.scala:167)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.$anonfun$genCodeWithFallback$2(EdgeExpressionCodegen.scala:248)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.$anonfun$genCodeWithFallback$1(EdgeExpressionCodegen.scala:248)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.genCodeWithFallback(EdgeExpressionCodegen.scala:246)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.generateExpression(CodeGenerator.scala:1285)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.$anonfun$generateExpressionsForWholeStageWithCSE$2(CodeGenerator.scala:1355)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.$anonfun$generateExpressionsForWholeStageWithCSE$1(CodeGenerator.scala:1353)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.withSubExprEliminationExprs(CodeGenerator.scala:1161)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.generateExpressionsForWholeStageWithCSE(CodeGenerator.scala:1353)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:70)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:195)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:150)\n\tat org.apache.spark.sql.execution.RDDScanExec.consume(ExistingRDD.scala:139)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:484)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:457)\n\tat org.apache.spark.sql.execution.RDDScanExec.doProduce(ExistingRDD.scala:139)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:96)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:241)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:237)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:91)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:91)\n\tat org.apache.spark.sql.execution.RDDScanExec.produce(ExistingRDD.scala:139)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:53)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:96)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:241)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:237)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:91)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:91)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:657)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:197)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:241)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:237)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:193)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:79)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:88)\n\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:75)\n\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:62)\n\tat org.apache.spark.sql.execution.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:512)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:511)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:399)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollectResult(limit.scala:59)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3018)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3810)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2742)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3802)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:126)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:267)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:852)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:217)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3800)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2742)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2949)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:306)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:343)\n\tat sun.reflect.GeneratedMethodAccessor423.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n</div>", "errorSummary": "java.lang.UnsupportedOperationException: Cannot generate code for expression: input[1, array&lt;string&gt;, true] DESC NULLS LAST", "metadata": {}, "errorTraceType": "html", "type": "ipynbError", "arguments": {}}}, "data": {"text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1184490609996896&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>functions <span class=\"ansi-green-fg\">import</span> sort_array<span class=\"ansi-blue-fg\">,</span> col\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>itemsDf<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;attributes&#39;</span><span class=\"ansi-blue-fg\">,</span> sort_array<span class=\"ansi-blue-fg\">(</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;attributes&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>desc<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">show</span><span class=\"ansi-blue-fg\">(self, n, truncate, vertical)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    488</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    489</span>         <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">,</span> bool<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">and</span> truncate<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 490</span><span class=\"ansi-red-fg\">             </span>print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">20</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    491</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    492</span>             print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> int<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 117</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o368.showString.\n: java.lang.UnsupportedOperationException: Cannot generate code for expression: input[1, array&lt;string&gt;, true] DESC NULLS LAST\n\tat org.apache.spark.sql.catalyst.expressions.Unevaluable.doGenCode(Expression.scala:365)\n\tat org.apache.spark.sql.catalyst.expressions.Unevaluable.doGenCode$(Expression.scala:364)\n\tat org.apache.spark.sql.catalyst.expressions.SortOrder.doGenCode(SortOrder.scala:62)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCodeInternal(Expression.scala:181)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$2(Expression.scala:160)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:160)\n\tat org.apache.spark.sql.catalyst.expressions.BinaryExpression.nullSafeCodeGen(Expression.scala:666)\n\tat org.apache.spark.sql.catalyst.expressions.SortArray.doGenCode(collectionOperations.scala:937)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCodeInternal(Expression.scala:181)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.$anonfun$genCodeWithFallback$2(EdgeExpressionCodegen.scala:248)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.$anonfun$genCodeWithFallback$1(EdgeExpressionCodegen.scala:248)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.genCodeWithFallback(EdgeExpressionCodegen.scala:246)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.doGenCode(EdgeExpressionCodegen.scala:1454)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.genCode(EdgeExpressionCodegen.scala:193)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:153)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.genCodeInternal(namedExpressions.scala:167)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.$anonfun$genCodeWithFallback$2(EdgeExpressionCodegen.scala:248)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.$anonfun$genCodeWithFallback$1(EdgeExpressionCodegen.scala:248)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.genCodeWithFallback(EdgeExpressionCodegen.scala:246)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.generateExpression(CodeGenerator.scala:1285)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.$anonfun$generateExpressionsForWholeStageWithCSE$2(CodeGenerator.scala:1355)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.$anonfun$generateExpressionsForWholeStageWithCSE$1(CodeGenerator.scala:1353)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.withSubExprEliminationExprs(CodeGenerator.scala:1161)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.generateExpressionsForWholeStageWithCSE(CodeGenerator.scala:1353)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:70)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:195)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:150)\n\tat org.apache.spark.sql.execution.RDDScanExec.consume(ExistingRDD.scala:139)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:484)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:457)\n\tat org.apache.spark.sql.execution.RDDScanExec.doProduce(ExistingRDD.scala:139)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:96)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:241)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:237)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:91)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:91)\n\tat org.apache.spark.sql.execution.RDDScanExec.produce(ExistingRDD.scala:139)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:53)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:96)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:241)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:237)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:91)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:91)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:657)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:197)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:241)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:237)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:193)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:79)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:88)\n\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:75)\n\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:62)\n\tat org.apache.spark.sql.execution.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:512)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:511)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:399)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollectResult(limit.scala:59)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3018)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3810)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2742)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3802)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:126)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:267)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:852)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:217)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3800)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2742)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2949)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:306)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:343)\n\tat sun.reflect.GeneratedMethodAccessor423.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n</div>"]}}], "execution_count": 0}, {"cell_type": "markdown", "source": ["## Answer 5 (incorrect)"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "8fef6132-53bf-4c40-b876-e17972c28fe8"}}}, {"cell_type": "code", "source": ["from pyspark.sql.functions import sort_array, desc\n\nitemsDf.withColumn('attributes', sort_array(desc('attributes'))).show()"], "metadata": {"application/vnd.databricks.v1+cell": {"title": "", "showTitle": false, "inputWidgets": {}, "nuid": "4a1dfb7c-aaf8-4c46-89c0-7367cb77a413"}}, "outputs": [{"output_type": "display_data", "metadata": {"application/vnd.databricks.v1+output": {"datasetInfos": [], "data": "<div class=\"ansiout\"></div>", "removedWidgets": [], "addedWidgets": {}, "metadata": {}, "type": "html", "arguments": {}}}, "data": {"text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}, {"output_type": "display_data", "metadata": {"application/vnd.databricks.v1+output": {"data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1184490609996894&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>functions <span class=\"ansi-green-fg\">import</span> sort_array<span class=\"ansi-blue-fg\">,</span> desc\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>itemsDf<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;attributes&#39;</span><span class=\"ansi-blue-fg\">,</span> sort_array<span class=\"ansi-blue-fg\">(</span>desc<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;attributes&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">show</span><span class=\"ansi-blue-fg\">(self, n, truncate, vertical)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    488</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    489</span>         <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">,</span> bool<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">and</span> truncate<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 490</span><span class=\"ansi-red-fg\">             </span>print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">20</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    491</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    492</span>             print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> int<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 117</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o346.showString.\n: java.lang.UnsupportedOperationException: Cannot generate code for expression: input[1, array&lt;string&gt;, true] DESC NULLS LAST\n\tat org.apache.spark.sql.catalyst.expressions.Unevaluable.doGenCode(Expression.scala:365)\n\tat org.apache.spark.sql.catalyst.expressions.Unevaluable.doGenCode$(Expression.scala:364)\n\tat org.apache.spark.sql.catalyst.expressions.SortOrder.doGenCode(SortOrder.scala:62)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCodeInternal(Expression.scala:181)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$2(Expression.scala:160)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:160)\n\tat org.apache.spark.sql.catalyst.expressions.BinaryExpression.nullSafeCodeGen(Expression.scala:666)\n\tat org.apache.spark.sql.catalyst.expressions.SortArray.doGenCode(collectionOperations.scala:937)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCodeInternal(Expression.scala:181)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.$anonfun$genCodeWithFallback$2(EdgeExpressionCodegen.scala:248)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.$anonfun$genCodeWithFallback$1(EdgeExpressionCodegen.scala:248)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.genCodeWithFallback(EdgeExpressionCodegen.scala:246)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.doGenCode(EdgeExpressionCodegen.scala:1454)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.genCode(EdgeExpressionCodegen.scala:193)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:153)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.genCodeInternal(namedExpressions.scala:167)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.$anonfun$genCodeWithFallback$2(EdgeExpressionCodegen.scala:248)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.$anonfun$genCodeWithFallback$1(EdgeExpressionCodegen.scala:248)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.genCodeWithFallback(EdgeExpressionCodegen.scala:246)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.generateExpression(CodeGenerator.scala:1285)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.$anonfun$generateExpressionsForWholeStageWithCSE$2(CodeGenerator.scala:1355)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.$anonfun$generateExpressionsForWholeStageWithCSE$1(CodeGenerator.scala:1353)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.withSubExprEliminationExprs(CodeGenerator.scala:1161)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.generateExpressionsForWholeStageWithCSE(CodeGenerator.scala:1353)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:70)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:195)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:150)\n\tat org.apache.spark.sql.execution.RDDScanExec.consume(ExistingRDD.scala:139)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:484)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:457)\n\tat org.apache.spark.sql.execution.RDDScanExec.doProduce(ExistingRDD.scala:139)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:96)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:241)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:237)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:91)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:91)\n\tat org.apache.spark.sql.execution.RDDScanExec.produce(ExistingRDD.scala:139)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:53)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:96)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:241)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:237)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:91)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:91)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:657)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:197)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:241)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:237)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:193)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:79)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:88)\n\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:75)\n\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:62)\n\tat org.apache.spark.sql.execution.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:512)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:511)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:399)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollectResult(limit.scala:59)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3018)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3810)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2742)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3802)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:126)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:267)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:852)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:217)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3800)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2742)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2949)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:306)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:343)\n\tat sun.reflect.GeneratedMethodAccessor423.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n</div>", "errorSummary": "java.lang.UnsupportedOperationException: Cannot generate code for expression: input[1, array&lt;string&gt;, true] DESC NULLS LAST", "metadata": {}, "errorTraceType": "html", "type": "ipynbError", "arguments": {}}}, "data": {"text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1184490609996894&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>functions <span class=\"ansi-green-fg\">import</span> sort_array<span class=\"ansi-blue-fg\">,</span> desc\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>itemsDf<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;attributes&#39;</span><span class=\"ansi-blue-fg\">,</span> sort_array<span class=\"ansi-blue-fg\">(</span>desc<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;attributes&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">show</span><span class=\"ansi-blue-fg\">(self, n, truncate, vertical)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    488</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    489</span>         <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">,</span> bool<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">and</span> truncate<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 490</span><span class=\"ansi-red-fg\">             </span>print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">20</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    491</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    492</span>             print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> int<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 117</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o346.showString.\n: java.lang.UnsupportedOperationException: Cannot generate code for expression: input[1, array&lt;string&gt;, true] DESC NULLS LAST\n\tat org.apache.spark.sql.catalyst.expressions.Unevaluable.doGenCode(Expression.scala:365)\n\tat org.apache.spark.sql.catalyst.expressions.Unevaluable.doGenCode$(Expression.scala:364)\n\tat org.apache.spark.sql.catalyst.expressions.SortOrder.doGenCode(SortOrder.scala:62)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCodeInternal(Expression.scala:181)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$2(Expression.scala:160)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:160)\n\tat org.apache.spark.sql.catalyst.expressions.BinaryExpression.nullSafeCodeGen(Expression.scala:666)\n\tat org.apache.spark.sql.catalyst.expressions.SortArray.doGenCode(collectionOperations.scala:937)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCodeInternal(Expression.scala:181)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.$anonfun$genCodeWithFallback$2(EdgeExpressionCodegen.scala:248)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.$anonfun$genCodeWithFallback$1(EdgeExpressionCodegen.scala:248)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.genCodeWithFallback(EdgeExpressionCodegen.scala:246)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.doGenCode(EdgeExpressionCodegen.scala:1454)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.genCode(EdgeExpressionCodegen.scala:193)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:153)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.genCodeInternal(namedExpressions.scala:167)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.$anonfun$genCodeWithFallback$2(EdgeExpressionCodegen.scala:248)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.$anonfun$genCodeWithFallback$1(EdgeExpressionCodegen.scala:248)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.genCodeWithFallback(EdgeExpressionCodegen.scala:246)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.generateExpression(CodeGenerator.scala:1285)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.$anonfun$generateExpressionsForWholeStageWithCSE$2(CodeGenerator.scala:1355)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.$anonfun$generateExpressionsForWholeStageWithCSE$1(CodeGenerator.scala:1353)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.withSubExprEliminationExprs(CodeGenerator.scala:1161)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.generateExpressionsForWholeStageWithCSE(CodeGenerator.scala:1353)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:70)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:195)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:150)\n\tat org.apache.spark.sql.execution.RDDScanExec.consume(ExistingRDD.scala:139)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:484)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:457)\n\tat org.apache.spark.sql.execution.RDDScanExec.doProduce(ExistingRDD.scala:139)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:96)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:241)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:237)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:91)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:91)\n\tat org.apache.spark.sql.execution.RDDScanExec.produce(ExistingRDD.scala:139)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:53)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:96)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:241)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:237)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:91)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:91)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:657)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:197)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:241)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:237)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:193)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:79)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:88)\n\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:75)\n\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:62)\n\tat org.apache.spark.sql.execution.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:512)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:511)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:399)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollectResult(limit.scala:59)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3018)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3810)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2742)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3802)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:126)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:267)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:852)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:217)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3800)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2742)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2949)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:306)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:343)\n\tat sun.reflect.GeneratedMethodAccessor423.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n</div>"]}}], "execution_count": 0}], "metadata": {"application/vnd.databricks.v1+notebook": {"notebookName": "92", "dashboards": [], "notebookMetadata": {"pythonIndentUnit": 2}, "language": "python", "widgets": {}, "notebookOrigID": 1184490609996886}}, "nbformat": 4, "nbformat_minor": 0}